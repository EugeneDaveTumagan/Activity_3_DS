# -*- coding: utf-8 -*-
"""ACtivity 2 -Tumagan 4B.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19eYT6uNYCC2WsClKzoyHiQqOTCQd7voT
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import copy
import pickle

file_id = '1G-VKgBJk-zjoInVXxmHwXpNoMTXdfmng'
download_link = f'https://drive.google.com/uc?export=download&id={file_id}'

import requests
import pandas as pd

response = requests.get(download_link)
with open('dataset.csv', 'wb') as file:
    file.write(response.content)

df = pd.read_csv('dataset.csv')
df

df.head()

df.tail()

df.info()

"""# Contains some missing values"""

fig, axes = plt.subplots(nrows=1, ncols=4 ,figsize=(15,3) )
count = 0
for icol in df.dtypes[df.dtypes == 'float64'].index:
  sns.histplot(df[icol] ,ax=axes[count], kde=True)
  print(icol,(df[icol].skew()))
  count+=1

fig, axes = plt.subplots(nrows=1, ncols=4 ,figsize=(15,3) )
count = 0
for icol in df.dtypes[df.dtypes == 'float64'].index:
  sns.boxplot(df[icol] ,ax=axes[count]).set_title(icol)
  sns.stripplot(df[icol] ,ax=axes[count], jitter=True).set_title(icol)
  count+=1

"""## The features have approximately normal distribution. With Administration having slight negative skew.

## There are multiple 0 values in some features, but is within whisker's range. Could be valid? -- Assuming that its not an error.

## Profit contains an outlier but doesnt seem to influence much.

## No extreme outliers. Simple mean based processes are viable.
"""

plt.figure(figsize=(3,3))
sns.heatmap(df.isna())

# FILL NULL VALUES
df2 = copy.copy(df)
for icol in df2.columns[:3]:
  df2[icol] = df2[icol].fillna(df2[icol].mean())

df2.info()

"""# Missing values occurs only once in a row. Missing values are imputed with mean."""

from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(sparse_output=False)
ohe_df = pd.DataFrame(ohe.fit_transform(df2[['City']]))
ohe_df.columns = list(pd.get_dummies(df2[['City']]))
df3 = pd.concat([ohe_df, df2], axis=1).drop('City', axis=1)
df3.head()

plt.figure(figsize=(20,5))
sns.heatmap(df3.corr(), annot=True)

fig, axes = plt.subplots(nrows=1, ncols=8 ,figsize=(15,3) )
count = 0
for icol in df3.dtypes[df3.dtypes == 'float64'].index:
  sns.scatterplot(x=df3[icol], y=df3.Profit, ax=axes[count]).set_title(icol)
  count+=1

"""# R&D Spend and Marketing Spend is highly correlated to Profit."""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Using simplified k-fold cross validation is good for validating smaller dataset.
r2_scores = []
iterations = 300
def ml(X,y):
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
  sc = StandardScaler()
  columns = X_train.columns[4:].to_list()
  for col in columns:
    X_train[col] = sc.fit_transform(X_train[[col]])
    X_test[col] = sc.fit_transform(X_test[[col]])

  lr = LinearRegression()
  lr.fit(X_train,y_train)
  y_predict = lr.predict(X_test)
  r2_scores.append(r2_score(y_test, y_predict))
  return lr

print('R2 SCORES')
for x in range(iterations):
  ml(X = df3.iloc[:,:-1],y = df3.iloc[:,-1])
print('\n(all - target) var as features:')
print(pd.Series(r2_scores).describe())
r2_scores = []

for x in range(iterations):
  model = ml(X = df3[['R&D Spend', 'Marketing Spend']],y = df3.iloc[:,-1])
print('\nR&D and Marketing (High Correlations):')
print(pd.Series(r2_scores).describe())
r2_scores = []
# ---------------------------------------
pickle.dump(model, open('model.pkl','wb'))

model = pickle.load(open('model.pkl','rb'))

#-----------------------------
for x in range(iterations):
  ml(X = df3[['R&D Spend', 'Marketing Spend', 'Administration']],y = df3.iloc[:,-1])
print('\nR&D and Marketing and Administration:')
print(pd.Series(r2_scores).describe())
r2_scores = []

"""#Using R&D and Marketing as predictors is the best multiple linear regression model with r2 scores of:

###**92.3%** average
###**71.6%** min
###**98.4%** max
"""

